-- PostgreSQL initialization script for Streaming Analytics Platform
-- This script sets up the database schema for storing analytics data

-- Drop existing tables if they exist (for clean reinstall)
drop table if exists user_alerts

cascade;
drop table if exists ml_metrics

cascade;
drop table if exists user_behavior_patterns

cascade;
drop table if exists stream_processing_logs

cascade;

-- Create analytics database schema

-- Table for storing user behavior patterns from ML analysis
create table user_behavior_patterns (
   id               uuid primary key default gen_random_uuid(),
   user_id          varchar(255) not null,
   pattern_type     varchar(100) not null, -- NORMAL, SUSPICIOUS, HIGH_ACTIVITY, etc.
   confidence_score decimal(5,4) not null, -- ML confidence score 0-1
   features         jsonb not null, -- Feature vector as JSON
   cluster_id       integer, -- K-means cluster assignment
   created_at       timestamp default current_timestamp,
   updated_at       timestamp default current_timestamp
);

-- Create indexes for user_behavior_patterns
create index idx_user_behavior_user_id on
   user_behavior_patterns (
      user_id
   );
create index idx_user_behavior_pattern_type on
   user_behavior_patterns (
      pattern_type
   );
create index idx_user_behavior_created_at on
   user_behavior_patterns (
      created_at
   );

-- Table for storing alerts generated by Flink CEP
create table user_alerts (
   id          uuid primary key default gen_random_uuid(),
   alert_type  varchar(100) not null, -- SUSPICIOUS_LOGIN, RAPID_ACTIVITY, etc.
   user_id     varchar(255) not null,
   severity    varchar(20) not null, -- LOW, MEDIUM, HIGH, CRITICAL
   message     text not null,
   metadata    jsonb, -- Additional alert context
   is_resolved boolean default false,
   resolved_at timestamp,
   resolved_by varchar(255),
   created_at  timestamp default current_timestamp
);

-- Create indexes for user_alerts
create index idx_user_alerts_user_id on
   user_alerts (
      user_id
   );
create index idx_user_alerts_type on
   user_alerts (
      alert_type
   );
create index idx_user_alerts_severity on
   user_alerts (
      severity
   );
create index idx_user_alerts_created_at on
   user_alerts (
      created_at
   );
create index idx_user_alerts_unresolved on
   user_alerts (
      is_resolved,
      created_at
   );

-- Table for storing ML model metrics and performance
create table ml_metrics (
   id                 uuid primary key default gen_random_uuid(),
   model_name         varchar(100) not null,
   metric_name        varchar(100) not null, -- accuracy, precision, recall, f1_score, etc.
   metric_value       decimal(10,6) not null,
   training_data_size integer,
   feature_count      integer,
   model_version      varchar(50),
   created_at         timestamp default current_timestamp
);

-- Create indexes for ml_metrics
create index idx_ml_metrics_model on
   ml_metrics (
      model_name
   );
create index idx_ml_metrics_name on
   ml_metrics (
      metric_name
   );
create index idx_ml_metrics_created_at on
   ml_metrics (
      created_at
   );

-- Table for stream processing logs and system metrics
create table stream_processing_logs (
   id                        uuid primary key default gen_random_uuid(),
   component                 varchar(50) not null, -- SPARK, FLINK, KAFKA
   log_level                 varchar(20) not null, -- INFO, WARN, ERROR
   message                   text not null,
   metadata                  jsonb, -- Additional structured logging data
   processing_time_ms        bigint, -- Processing time for performance tracking
   throughput_events_per_sec decimal(10,2),
   created_at                timestamp default current_timestamp
);

-- Create indexes for stream_processing_logs
create index idx_stream_logs_component on
   stream_processing_logs (
      component
   );
create index idx_stream_logs_level on
   stream_processing_logs (
      log_level
   );
create index idx_stream_logs_created_at on
   stream_processing_logs (
      created_at
   );

-- Create simple views for dashboard analytics (without complex time filtering for compatibility)

-- User activity summary
create view user_activity_summary as
   select date_trunc(
      'hour',
      created_at
   ) as hour,
          count(distinct user_id) as unique_users,
          count(*) as total_events,
          avg(confidence_score) as avg_confidence,
          count(
             case
                when pattern_type = 'SUSPICIOUS' then
                   1
             end
          ) as suspicious_events
     from user_behavior_patterns
    group by date_trunc(
      'hour',
      created_at
   )
    order by hour desc;

-- Alert summary by severity
create view alert_summary as
   select date_trunc(
      'hour',
      created_at
   ) as hour,
          severity,
          count(*) as alert_count,
          count(
             case
                when is_resolved then
                   1
             end
          ) as resolved_count
     from user_alerts
    group by date_trunc(
      'hour',
      created_at
   ),
             severity
    order by hour desc,
             severity;

-- System performance metrics
create view system_performance as
   select date_trunc(
      'minute',
      created_at
   ) as minute,
          component,
          avg(processing_time_ms) as avg_processing_time_ms,
          avg(throughput_events_per_sec) as avg_throughput,
          count(
             case
                when log_level = 'ERROR' then
                   1
             end
          ) as error_count
     from stream_processing_logs
    group by date_trunc(
      'minute',
      created_at
   ),
             component
    order by minute desc,
             component;

-- Insert some initial sample data for testing
insert into user_behavior_patterns (
   user_id,
   pattern_type,
   confidence_score,
   features,
   cluster_id
) values ( 'user_001',
           'NORMAL',
           0.95,
           '{"login_frequency": 3, "avg_session_duration": 1800, "unique_locations": 1}',
           1 ),( 'user_002',
                 'SUSPICIOUS',
                 0.87,
                 '{"login_frequency": 15, "avg_session_duration": 300, "unique_locations": 5}',
                 3 ),( 'user_003',
                       'HIGH_ACTIVITY',
                       0.92,
                       '{"login_frequency": 8, "avg_session_duration": 3600, "unique_locations": 2}',
                       2 ),( 'user_004',
                             'NORMAL',
                             0.98,
                             '{"login_frequency": 2, "avg_session_duration": 2400, "unique_locations": 1}',
                             1 );

insert into user_alerts (
   alert_type,
   user_id,
   severity,
   message,
   metadata
) values ( 'SUSPICIOUS_LOGIN',
           'user_002',
           'HIGH',
           'Multiple failed login attempts detected within 5 minutes',
           '{"failed_attempts": 5, "time_window_minutes": 5}' ),( 'RAPID_ACTIVITY',
                                                                  'user_003',
                                                                  'MEDIUM',
                                                                  'Unusually high activity detected - 50 events in 1 minute',
                                                                  '{"event_count": 50, "time_window_minutes": 1}' ),( 'LOCATION_ANOMALY'
                                                                  ,
                                                                                                                      'user_002'
                                                                                                                      ,
                                                                                                                      'HIGH',
                                                                                                                      'Login from new geographic location'
                                                                                                                      ,
                                                                                                                      '{"new_location": "Unknown", "previous_location": "New York"}'
                                                                                                                      );

insert into ml_metrics (
   model_name,
   metric_name,
   metric_value,
   training_data_size,
   feature_count,
   model_version
) values ( 'UserBehaviorKMeans',
           'silhouette_score',
           0.75,
           10000,
           15,
           '1.0' ),( 'UserBehaviorKMeans',
                     'inertia',
                     1250.45,
                     10000,
                     15,
                     '1.0' ),( 'AnomalyDetectionModel',
                               'precision',
                               0.89,
                               5000,
                               20,
                               '1.0' ),( 'AnomalyDetectionModel',
                                         'recall',
                                         0.82,
                                         5000,
                                         20,
                                         '1.0' ),( 'AnomalyDetectionModel',
                                                   'f1_score',
                                                   0.85,
                                                   5000,
                                                   20,
                                                   '1.0' );

insert into stream_processing_logs (
   component,
   log_level,
   message,
   processing_time_ms,
   throughput_events_per_sec
) values ( 'SPARK',
           'INFO',
           'Spark streaming job started successfully',
           1500,
           2340.5 ),( 'FLINK',
                      'INFO',
                      'Flink CEP pattern matching initialized',
                      800,
                      1890.2 ),( 'KAFKA',
                                 'INFO',
                                 'Kafka consumer connected to topic: user-events',
                                 200,
                                 5000.0 ),( 'SPARK',
                                            'WARN',
                                            'High memory usage detected in Spark executor',
                                            2100,
                                            1200.0 ),( 'FLINK',
                                                       'ERROR',
                                                       'Temporary checkpoint failure - recovering',
                                                       3000,
                                                       0.0 );

-- Create additional indexes for better query performance
create index idx_user_behavior_patterns_composite on
   user_behavior_patterns (
      user_id,
      created_at,
      pattern_type
   );

create index idx_user_alerts_composite on
   user_alerts (
      user_id,
      created_at,
      is_resolved
   );

create index idx_ml_metrics_composite on
   ml_metrics (
      model_name,
      metric_name,
      created_at
   );

-- Print confirmation message
select 'Analytics database schema initialized successfully' as status;